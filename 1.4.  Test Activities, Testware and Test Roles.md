[[Testing]] is context dependent, but, at a high level, there are common sets of test activities without which [[Testing | testing]] is less likely to achieve [[Test Objective | test objectives]].  These sets of test activities form a [[Test Process | test process]].  The [[Test Process | test process]] can be tailored to a given situation based on various factors.  Which test activities are included in this test process, how they are implemented, and when they occur is normally decided as part of the [[Test Planning | test planning]] for the specific situation (see [[5.1.  Test Planning | section 5.1]]).

The following sections describe the general aspects of this [[Test Process | test process]] in terms of test activities and tasks, the impact of context, [[Testware | testware]], [[Traceability | traceability]] between the [[Test Basis | test basis]] and [[Testware | testware]], and testing roles.

The [[ISO-IEC-IEEE 29119-2 | ISO/IEC/IEEE 29119-2 standard]] provides further information about [[Test Process | test processes]].

## 1.4.1. Test Activities and Tasks

A [[Test Process | test process]] usually consists of the main groups of activities described below.  Although many of these activities may appear to follow a logical sequence, they are often implemented iteratively or in parallel.  These testing activities usually need to be tailored to the system and the project.

**[[Test Planning]]** consists of defining the [[Test Objective | test objectives]] and then selecting an approach that best achieves the objectives within the constraints imposed by the overall context.  [[Test Planning | Test planning]] is further explained in [[5.1.  Test Planning | section 5.1]].

**[[Test Monitoring | Test monitoring]] and [[Test Control | test control]]**.  [[Test Monitoring | Test monitoring]] involves the ongoing checking of all test activities and the comparison of actual progress against the plan.  [[Test Control | Test control]] involves taking the actions necessary to meet the [[Test Objective | test objectives]].  [[Test Monitoring | Test monitoring]] and [[Test Control | test control]] are further explained in [[5.3.  Test Monitoring, Test Control and Test Completion | section 5.3]].

**[[Test Analysis | Test analysis]]** includes analyzing the [[Test Basis | test basis]] to identify testable features.  Associated [[Test Condition | test conditions]] are defined and prioritized, taking the related [[Risk | risks]] and [[Risk Level | risk levels]] into account (see [[5.2.  Risk Management | section 5.2]]).  The [[Test Basis | test basis]] and the [[Test Object | test object]] are also evaluated to identify [[Defect | defects]] they may contain and to assess their testability.  [[Test Analysis | Test analysis]] is often supported by the use of [[Test Technique | test techniques]] (see [[4.0.  Test Analysis and Design | chapter 4]]).  [[Test Analysis | Test analysis]] answers the question "what to test?" in terms of measurable coverage criteria.

**[[Test Design | Test design]]** includes elaborating the [[Test Condition | test conditions]] into [[Test Case | test cases]] and other [[Testware | testware]] (e.g., test charters).  This activity often involves the identification of [[Coverage Item | coverage items]], which serve as a guide to specify [[Test Case | test cases]] inputs.  [[Test Technique | Test techniques]] (see [[4.0.  Test Analysis and Design | chapter 4]]) can be used to support this activity.  [[Test Design | Test design]] also includes defining the test data requirements, designing the test environment and identifying the necessary infrastructure and tools.  [[Test Design | Test design]] answers the question "how to test?".

**[[Test Implementation | Test implementation]]** includes creating or acquiring the [[Testware | testware]] necessary for [[Test Execution | test execution]] (e.g., [[Test Data | test data]]).  [[Test Case | Test cases]] can be organized into [[Test Procedure | test procedures]], which are often assembled into test suites.  Manual and automated test scripts are created.  [[Test Procedure | Test procedures]] are prioritized and arranged within a [[Test Execution | test execution]] schedule for efficient test execution (see [[5.1.  Test Planning#5.1.5. Test Case Prioritization | section 5.1.5]]).  The test environment is built and verified to be set up correctly.

**[[Test Execution | Test execution]]** includes running the tests in accordance with the [[Test Execution | test execution]] schedule (test runs).  [[Test Execution | Test execution]] may be manual or automated.  [[Test Execution | Test execution]] can take many forms, including continuous testing or pair testing sessions.  Actual [[Test Result | test results]] are compared with the expected results.  The [[Test Result | test results]] are logged.  Anomalies are analyzed to identify their likely causes.  This analysis allows us to report the anomalies based on the [[Failure | failures]] observed (see [[5.5.  Defect Management | section 5.5]]).

**[[Test Completion | Test completion]]** usually occurs at project milestones (e.g., release, end of iteration, [[Test Level | test level]] completion).  For any unresolved [[Defect | defects]], change requests or product backlog items are created.  Any [[Testware | testware]] that may be useful in the future is identified and archived or handed over to the appropriate teams.  The test environment is shut down to an agreed state.  The test activities are analyzed to identify lessons learned and improvements for future iterations, releases, or projects (see [[2.1.  Testing in the Context of a Software Development Lifecycle (SDLC)#2.1.6. Retrospectives and Process Improvement | section 2.1.6]]).  A [[Test Completion | test completion]] report is created and communicated to the stakeholders.

## 1.4.2.  Test Process in Context

[[Testing]] is not performed in isolation.  Test activities are an integral part of the development processes carried out within an organization.  [[Testing]] is also funded by stakeholders and its final goal is to help fulfill the stakeholders' business needs.  Therefore, the way the [[Testing | testing]] is carried out will depend on a number of contextual factors including:

* Stakeholders (needs, expectations, requirements, willingness to cooperate, etc.)
* Team members (skills, knowledge, level of experience, availability, training needs, etc.)
* Business domain (criticality of the [[Test Object | test object]], identified [[Risk | risks]], market needs, specific legal regulations, etc.)
* Technical factors (type of software, product architecture, technology used, etc.)
* Project constraints (scope, time, budget, resources, etc.)
* Organizational factors (organizational structure, existing policies, practices used, etc.)
* Software development lifecycle (engineering practices, development methods, etc.)
* Tools (availability, usability, compliance, etc.)

These factors will have an impact on many test-related issues, including: [[Test Strategy | test strategy]], [[Test Technique | test techniques]] used, degree of [[Test Automation | test automation]], required level of [[Coverage | coverage]], level of detail of [[Testware | testware]], test reporting, etc.

## 1.4.3.  Testware

[[Testware]] is created as output work products from the test activities described in [[1.4.  Test Activities, Testware and Test Roles#1.4.1. Test Activities and Tasks | section 1.4.1]].  There is a significant variation in how different organizations produce, shape, name, organize and manager their work products.  Proper configuration management (see [[5.4.  Configuration Management | section 5.4]]) ensures consistency and integrity of work products.  The following list of work products is not exhaustive:

* **[[Test Planning]] work product** include: [[Test Plan | test plan]], test schedule, risk register, [[Entry Criteria | entry criteria]] and [[Exit Criteria | exit criteria]] (see [[5.1.  Test Planning | section 5.1]]).  Risk register is a list of [[Risk | risks]] together with risk likelihood, risk impact and information about [[Risk Mitigation | risk mitigation]] (see [[5.2.  Risk Management | section 5.2]]).  Test schedule, risk register, [[Entry Criteria | entry criteria]] and [[Exit Criteria | exit criteria]] are often a part of the [[Test Plan | test plan]].
* **[[Test Monitoring | Test monitoring]] and [[Test Control | test control]] work products** include: [[Test Progress Report | test progress reports]] (see [[5.3.  Test Monitoring, Test Control and Test Completion#5.3.2. Purpose, Content and Audience for Test | section 5.3.2]]), documentation of control directives (see [[5.3.  Test Monitoring, Test Control and Test Completion | section 5.3]]) and information about risks (see [[5.2.  Risk Management | section 5.2]]).
* **[[Test Analysis | Test analysis]] work products** include: (prioritized) [[Test Condition | test conditions]] (e.g., [[Acceptance Criteria | acceptance criteria]], see [[4.5.  Collaboration-based Test Approaches#4.5.2. Acceptance Criteria | section 4.5.2]]), and [[Defect Report | defect reports]] regarding [[Defect | defects]] in the [[Test Basis | test basis]] (if not fixed directly).
* **[[Test Design | Test design]] work products** include: (prioritized) [[Test Case | test cases]], test charters, [[Coverage Item | coverage items]], [[Test Data | test data]] requirements and test environment requirements.
* **[[Test Implementation | Test implementation]] work products** include: [[Test Procedure | test procedures]], manual and automated test scripts, test suites, [[Test Data | test data]], [[Test Execution | test execution]] schedule, and test environment items.  Examples of test environment items include: stubs, drivers, simulators, and service virtualizations.
* **[[Test Execution]] work products** include: test logs, and [[Defect Report | defect reports]] (see [[5.5.  Defect Management | section 5.5]]).
* **[[Test Completion | Test completion]] work products** include: [[Test Completion Report | test completion report]] (see [[5.3.  Test Monitoring, Test Control and Test Completion#5.3.2. Purpose, Content and Audience for Test | section 5.3.2]]), action items for improvement of subsequent projects or iterations, documented lessons learned, and change requests (e.g., as product backlog items).

## 1.4.4.  Traceability between the Test Basis and Testware

To implement effective [[Test Monitoring | test monitoring]] and [[Test Control | test control]], it is important to establish and maintain [[Traceability | traceability]] throughout the [[Test Process | test process]] between the [[Test Basis | test basis]] elements, [[Testware | testware]] associated with these elements (e.g., [[Test Condition | test conditions]], [[Risk | risk]], [[Test Case | test cases]]), [[Test Result | test results]], and [[Defect | defects]].

Accurate [[Traceability | traceability]] supports [[Coverage | coverage]] evaluation, so it is very useful if measurable coverage criteria are defined in the [[Test Basis | test basis]].  The coverage criteria can function as key performance indicators to drive the activities that show to what extent the [[Test Objective | test objectives]] have been achieved (see [[1.1.  What is Testing?#1.1.1. Test Objectives | section 1.1.1]]).  For example:

* [[Traceability]] of [[Test Case | test cases]] to requirements can verify that the requirements are covered by [[Test Case | test cases]]. 
* [[Traceability]] of [[Test Result | test results]] to risks can be used to evaluate the level of residual [[Risk | risk]] in a [[Test Object | test object]].

In addition to evaluating [[Coverage | coverage]], good [[Traceability | traceability]] makes it possible to determine the impact of changes, facilitates audits, and helps meet IT governance criteria.  Good [[Traceability | traceability]] also makes [[Test Progress Report | test progress reports]] and [[Test Completion Report | test completion reports]] more easily understandable by including the status of [[Test Basis | test basis]] elements.  This can also assist in communicating the technical aspects of [[Testing | testing]] to stakeholders in an understandable manner.  [[Traceability]] provides information to assess product quality, process capability, and project progress against business goals.

## 1.4.5.  Roles in Testing

In this syllabus, two principal roles in [[Testing | testing]] are covered: a test management role and a testing role.  The activities and tasks assigned to these two roles depend on factors such as the project and product context, the skills of the people in the roles, and the organization.

The test management role takes overall responsibility for the [[Test Process | test process]], test team and leadership of the test activities.  The test management role is mainly focused on the activities of [[Test Planning | test planning]], [[Test Monitoring | test monitoring]], [[Test Control | test control]] and [[Test Completion | test completion]]. The way in which the test management role is carried out varies depending on the context.  For example, in Agile software development, some of the test management tasks may be handled by the Agile team.  Tasks that span multiple teams or the entire organization may be performed by test managers outside of the development team.

The testing role takes overall responsibility for the engineering (technical) aspect of [[Testing | testing]].  The testing role is mainly focused on the activities of [[Test Analysis | test analysis]], [[Test Design | test design]], [[Test Implementation | test implementation]] and [[Test Execution | test execution]].

Different people may take on these roles at different times.  For example, the test management role can be performed by a team leader, by a test manager, by a development manager, etc.  Its is also possible for one person to take on the roles of testing and test management at the same time.

---
<< [[1.3.  Testing Principles | Previous Section]] | [[1.5.  Essential Skills and Good Practices in Testing | Next Section]] >>