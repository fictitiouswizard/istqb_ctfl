
[[Test Level | Test levels]] are groups of test activities that are organized and managed together.  Each [[Test Level | test level]] is an instance of the [[Test Process | test process]], performed in relation to software at a given phase of development, from individual components to complete systems or, where applicable, systems of systems.

[[Test Level | Test levels]] are related to other activities within the [[SDLC]].  In sequential [[SDLC]] models,  the [[Test Level | test levels]] are often defined such that the [[Exit Criteria | exit criteria]] of one level are part of the [[Entry Criteria | entry criteria]] for the next level.  In some iterative models,  this may not apply.  Development activities may span through multiple [[Test Level | test levels]].  [[Test Level | Test levels]] may overlap in time.

[[Test Type | Test types]] are groups of test activities related to specific quality characteristics and most of those test activities can be performed at every [[Test Level | test level]].

##  2.2.1.  Test Levels

In this syllabus, the following five test levels are described:

* **[[Component Testing | Component testing]]** (also known as unit testing) focuses on testing components in isolation.  It often requires specific support, such as test harnesses or unit test frameworks.  [[Component Testing | Component testing]] is normally performed by developers in their development environments.
* **[[Component Integration Testing | Component Integration testing]]** (also known as unit integration testing) focuses on testing the interfaces and interactions between components.  [[Component Integration Testing | Component integration testing]] is heavily dependent on the integration strategy like bottom-up, top-down or big-bang.
* **[[System Testing | System testing]]** focuses on the overall behavior and capabilities of an entire system or product, often including [[Functional Testing | functional testing]] of end-to-end tasks and the [[Non-functional Testing | non-functional testing]] of quality characteristics.  For some non-functional quality characteristics, it is preferable to test them on a complete system in a representative test environment (e.g., usability).  Using simulations of sub-systems is also possible.  [[System Testing | System testing]] may be performed by an independent test team, and is related to specifications for the system.
* **[[System Integration Testing | System integration testing]]** focuses on [[Testing | testing ]] the interfaces of the system under test and other systems and external services.  [[System Integration Testing | System integration testing]] requires suitable test environments preferably similar to the operational environment.
* **[[Acceptance Testing | Acceptance testing]]** focuses on validation and on demonstrating readiness for deployment, which means that the system fulfills the user's business needs.  Ideally, [[Acceptance Testing | acceptance testing]] should be performed by the intended users.  The main forms of [[Acceptance Testing | acceptance testing]] are: user acceptance testing (UAT), operational acceptance testing, contractual acceptance testing and regulatory acceptance testing, alpha testing and beta testing.

[[Test Level | Test levels]] are distinguished by the following non-exhaustive list of attributes, to avoid overlapping of test activities:

* [[Test Object | Test object]]
* [[Test Objective | Test objective]]
* [[Test Basis | Test basis]]
* [[Defect | Defects]] and [[Failure | failures]]
* Approach and responsibilities

##  2.2.2.  Test Types

A lot of [[Test Type | test types]] exist and can be applied in projects.  In this syllabus, the following four test types are addressed:

**[[Functional Testing | Functional testing]]** evaluates the functions that a component or system should perform.  The functions are "what" the [[Test Object | test object]] should do.  The main objective of [[Functional Testing | functional testing]] is checking the functional completeness, functional correctness and functional appropriateness.

**[[Non-functional Testing | Non-functional testing]]** evaluates attributes other than functional characteristics of a component or system.  [[Non-functional Testing | Non-functional testing]] is the testing of "how well the system behaves".  The main objective of [[Non-functional Testing | non-functional testing]] is checking the non-functional quality characteristics.  The [[ISO-IEC 25010 | ISO/IEC 25010 standard]] provides the following classification of the non-functional quality characteristics:

* Performance efficiency
* Compatibility
* Usability (also known as interaction capability)
* Reliability
* Security
* Maintainability
* Portability (also known as flexibility)
* Safety

It is sometimes appropriate for [[Non-functional Testing | non-functional testing]] to start early in the [[SDLC]] (e.g., as part of reviews or component testing).  Many non-functional tests are derived from functional tests as they use the same functional tests, but check that while performing the function, a non-functional constraint is satisfied e.g., checking that a function performs within an specified time, or a function can be ported to a new platform).  The late discovery of non-functional [[Defect | defects]] can pose a serious threat to the success of a project.  [[Non-functional Testing | Non-functional testing]] sometimes needs a very specific test environment, such as a usability lab for usability testing.

**[[Black-Box Testing | Black-box testing]]** (see [[4.2.  Black-Box Test Techniques | section 4.2]]) is specification-based and derives tests from documentation not related to the internal structure of the [[Test Object | test object]].  The main objective of [[Black-Box Testing | black-box testing]] is checking the system's behavior against its specifications.

**[[White-Box Testing | White-box testing]]** (see [[4.3.  White-Box Test Techniques | section 4.3]]) is structure-based and derives tests from the system's implementation or internal structure (e.g., code, architecture, work flows, and data flows).  The main objective of [[White-Box Testing | white-box testing]] is to cover the underlying structure by the tests to an acceptable level.

All the four above mentioned [[Test Type | test types]] can be applied to all [[Test Level | test levels]], although the focus will be different at each level.  Different [[Test Technique | test techniques]] can be used to derive [[Test Condition | test conditions]] and [[Test Case | test cases]] for all the mentioned [[Test Type | test types]].

##  2.2.3.  Confirmation Testing and Regression Testing

Changes are typically made to a component or system to either enhance it by adding a new feature or to fix it by removing a [[Defect | defect]].  [[Testing]] should then also include [[Confirmation Testing | confirmation testing]] and [[Regression Testing | regression testing]].

**[[Confirmation Testing | Confirmation testing]]** confirms that an original [[Defect | defect]] has been successfully fixed.  Depending on the [[Risk | risk]], one can test the fixed version of the software in several ways including:

* executing all tests that previously have failed due to the [[Defect | defect]], or, also by
* adding new tests to cover any changes that were needed to fix the [[Defect | defect]]

However, when time or money is short when fixing [[Defect | defects]], [[Confirmation Testing | confirmation testing]] might be restricted to simply exercising the test steps that should reproduce the [[Failure | failure]] caused by the [[Defect | defect]] and checking that the [[Failure | failure]] does not occur.

**[[Regression Testing | Regression testing]]** confirms that no adverse consequences have been caused by a change, including a fix that has already been [[Confirmation Testing | confirmation tested]].  These adverse consequences could affect the same component where the change was made, other components in the same system, or even other connected systems.  [[Regression Testing | Regression testing]] may not be restricted to the [[Test Object | test object]] itself but can also be related to the environment.  It is advisable first to perform an impact analysis to recognize the extent of the [[Regression Testing | regression testing]].  Impact analysis shows which parts of the software could be affected.

Regression test suites are run many times and generally the number of regression test cases will increase with each iteration or release, so [[Regression Testing | regression testing]] is a strong candidate for automation.  [[Test Automation | Test automation]] should start early in the project.  Where CI is used, such as in DevOps (see [[2.1.  Testing in the Context of a Software Development Lifecycle (SDLC)#2.1.4. DevOps and Testing | section 2.1.4]] ), it is good practice to also include automated [[Regression Testing | regression tests]].  Depending on the situation, this may include regression tests on different [[Test Level | test levels]].

[[Confirmation Testing | Confirmation testing]] and/or [[Regression Testing | regression testing]] for the [[Test Object | test object]] are needed on all [[Test Level | test levels]] if [[Defect | defects]] are fixed and/or changes are made on these [[Test Level | test levels]].

---
<< [[2.1.  Testing in the Context of a Software Development Lifecycle (SDLC) | Previous Section]] | [[2.3.  Maintenance Testing | Next Section]] >>