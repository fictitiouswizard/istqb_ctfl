---
title: "5.1. Test Planning"
---

# 5.1. Test Planning

##  5.1.1.  Purpose and Content of a Test Plan

A [[Test Plan | test plan]] describes the [[Test Objective | test objectives]], resources and processes for a test project.  A [[Test Plan | test plan]]:

* Documents the means and schedule for achieving [[Test Objective | test objectives]]
* Helps to ensure that the performed test activities will meet the established criteria
* Serves as a means of communication with team members and other stakeholders
* Demonstrates that testing will adhere to the existing test policy and [[Test Strategy | test strategy]] (or explains whey the [[Testing | testing]] will deviate from them)

Test planning guides the tester's thinking and forces the testers to confront the future challenges related to [[Risk | risks]], schedules, people, tools, costs, effort, etc.  The process of preparing a [[Test Plan | test plan]] is a useful way to think through the efforts needed to achieve the [[Test Objective]].

The typical content of a [[Test Plan | test plan]] includes:

* Context of [[Testing | testing]] (e.g., test scope, [[Test Objective | test objectives]], [[Test Basis | test basis]])
* Assumptions and constraints of the test project
* Stakeholders (e.g., roles, responsibilities, relevance to testing, hiring and training needs)
* Communication (e.g., forms and frequency of communication, documentation templates)
* Risk register (e.g., [[Product Risk | product risks]], [[Project Risk | project risks]])
* Test approach (e.g., [[Test Level | test levels]], [[Test Type | test types]], [[Test Technique | test techniques]], test deliverables, [[Entry Criteria | entry criteria]] and [[Exit Criteria | exit criteria]], independence of [[Testing | testing]], metrics to be collected, [[Test Data | test data]] requirements, test environment requirements, deviations from the test policy and [[Test Strategy | test strategy]])
* Budget and schedule

More details about the test plan and its content can be found in the [[ISO-IEC-IEEE 29119-3 | ISO/IEC/IEEE 29119-3 standard]].

##  5.1.2.  Tester's Contribution to Iteration and Release Planning

In iterative [[SDLC | SDLC's]], typically two kinds of planning occur: release planning and iteration planning.

Release planning looks ahead to the release of a product, defines and re-defines the product backlog, and may involve refining larger user stories into a set of smaller user stories.  It also serves as the basis for the test approach and [[Test Plan | test plan]] across all iterations.  Testers involved in release planning participate in writing testable user stories and [[Acceptance Criteria | acceptance criteria]] (see [[4.5.  Collaboration-based Test Approaches | section 4.5]]), participants in project and quality [[Risk Analysis | risk analysis]] (see [[5.2.  Risk Management | section 5.2]]), estimate test effort associated with user stories (see [[5.1.  Test Planning#5.1.4. Estimation Techniques | section 5.1.4]]), determine the test approach, and plan the [[Testing | testing]] for release.

Iteration planning looks ahead to the end of a single iteration and is concerned with the iteration backlog.  Testers involved in iteration planning participate in the detailed [[Risk Analysis | risk analysis]] of user stories, determine the testability of user stories, break down user stories into tasks (particularly testing tasks), estimate test effort for all testing tasks, and identify and refine functional and non-functional aspects of the [[Test Object | test object]].

##  5.1.3.  Entry Criteria and Exit Criteria

[[Entry Criteria | Entry criteria]] define the preconditions for undertaking a given activity.  If [[Entry Criteria | entry criteria]] are not met, it is likely that the activity will prove to be more difficult, time-consuming, costly, and riskier.  [[Exit Criteria | Exit criteria]] define what must be achieved to declare an activity completed.  [[Entry Criteria | Entry criteria]] and [[Exit Criteria | exit criteria]] should be defined for each [[Test Level | test level]], and will differ based on the [[Test Objective | test objectives]].

Typical [[Entry Criteria | entry criteria]] include: availability of resources (e.g., people, tools, environments test data, budget, time), availability of [[Testware | testware]] (e.g., [[Test Basis | test basis]], testable requirements, user stories, [[Test Case | test cases]]), and initial quality level of [[Test Object | test object]] (e.g., all smoke tests have passed).

Typical [[Exit Criteria | exit criteria]] include: measures of thoroughness (e.g., achieved level of [[Coverage | coverage]], number of unresolved [[Defect | defects]], [[Defect | defect]] density, number of failed [[Test Case | test cases]]), and binary "yes/no" criteria (e.g., planned tests have been executed, [[Static Testing | static testing]] has been performed, all [[Defect | defects]] found are reported, all regression tests are automated).

Running out of time or budget can also be viewed as valid [[Exit Criteria | exit criteria]].  Even without other [[Exit Criteria | exit criteria]] being satisfied, it can be acceptable to end testing under such circumstances, if the stakeholders have reviewed and accepted the [[Risk | risk]] to go live without further [[Testing | testing]].

In Agile software development, [[Exit Criteria | exit criteria]] are often called Definition of Done, defining the team's objective metrics for a releasable item.  [[Entry Criteria | Entry criteria]] that a user story must fulfill to start the development and/or testing activities are called Definition of Ready.

##  5.1.4.  Estimation Techniques

Test effort estimation involves predicting the amount of test-related work needed to meet the [[Test Objective | test objectives]] of a test project.  It is important to make it clear to the stakeholders that the estimate is based on a number of assumptions and is always subject to estimation error.  Estimation for small tasks is usually more accurate than for the large ones.  Therefore, when estimating a large task, it can be decomposed into a set of smaller tasks which then in turn can be estimated.

In this syllabus, the following four estimation techniques are described.

**Estimation based on ratios**.  In this metrics-based technique, figures are collected from previous projects within the organization, which makes it possible to derive "standard" ratios for similar projects.  The ratios of an organization's own projects (e.g., taken from historical data) are generally the best source to use in the estimation process.  These standard ratios can then be used to estimate  the test effort for the new project.  For example, if in the previous project the development-to-test effort ratio was 3:2, and in the current project the development effort is expected to be 600 person-days, the test effort can be estimated to be 400 person-days.

**Extrapolation**.  In this metrics-based technique, measurements are made as early as possible in the current project to gather the data.  Having enough observations, the effort required for the remaining work can be approximated by extrapolating this data (usually by applying a mathematical model).  This method is very suitable in iterative [[SDLC | SDLCs]].  For example, the team may extrapolate the test effort in the forthcoming iteration as the averaged effort from the last three iterations.

**Wideband Delphi**.  In this iterative, expert-based technique, experts make experience-based estimations.  Each expert, in isolation, estimates the effort.  The results are collected and if there are deviations of an expert's estimate that are out of range of the agreed upon boundaries, the experts discuss their current estimates.  Each expert is then asked to make a new estimation based on that feedback, again in isolation.  This process is repeated until consensus is reached.  Planning Poker is a variant of Wideband Delphi, commonly used in Agile software development.  In Planning Poker, estimates are usually made using cards with numbers that represent the effort size.

**Three-point estimation**.  In this expert-based technique, three estimations are made by the experts: the most optimistic estimation (a), the most likely estimation (m) and the most pessimistic estimation (b).  The final estimate (E) is their weighted arithmetic mean.  In the most popular version of this technique, the estimate is calculated as $$E = (a + 4*m + b) / 6$$.  The advantage of this technique is that it allows the experts to calculate the measurement error: $$SD = (b - a) / 6$$.  For example, if the estimates (in person-hours) are: a=6, m=9 and b=18, then the final estimation is 10 + or - 2 person-hours (i.e., between 8 and 12 person-hours), because $$E = (6 + 4 * 9 + 18) / 6 = 10$$ and $$SD = (18 - 6) / 6 = 2$$.

See ([[Metrics and Models in Software Quality Engineering | Kan 2003]], [[TMap Next for result-driven testing | Koomen 2006]], [[The Certified Software Quality Engineer Handbook | Westfall 2009]]) for these and many other test estimation techniques.

##  5.1.5.  Test Case Prioritization

Once the [[Test Case | test cases]] and [[Test Procedure | test procedures]] are specified and assembled into test suites, these test suites can be arranged in a test execution schedule that defines the order in which they are to be run.  When prioritizing [[Test Case | test cases]], different factors can be taken into account.  The most commonly used [[Test Case | test case]] prioritization strategies are as follows:

* Risk-based prioritization, where the order of test execution is based on the results of [[Risk Analysis | risk analysis]] (see [[5.2.  Risk Management#5.2.3. Product Risk Analysis | section 5.2.3]]).  [[Test Case | Test cases]] covering the most important risks are executed first.
* Coverage-based prioritization, where the order of test execution is based on [[Coverage | coverage]] (e.g., [[Statement Coverage | statement coverage]]).  [[Test Case | Test cases]] achieving the highest [[Coverage | coverage]] are executed first.  In another variant, called additional coverage prioritization, the [[Test Case | test case]] achieving the highest [[Coverage | coverage]] is executed first; each subsequent [[Test Case | test case]] is the one that achieves the highest additional [[Coverage | coverage]].
* Requirements-based prioritization, where the order of test execution is based on the priorities of the requirements traced back to the corresponding [[Test Case | test cases]].  Requirement priorities are defined by stakeholders.  [[Test Case | Test cases]] related to the most important requirements are executed first.

Ideally, [[Test Case | test cases]] would be ordered to run based on their priority levels, using, for example, one of the above-mentioned prioritization strategies.  However, this practice may not work if the [[Test Case | test cases]] or the features being tested have dependencies.  If a [[Test Case | test case]] with a higher priority is dependent on a [[Test Case | test case]] with a lower priority, the lower priority [[Test Case | test case]] must be executed first.

The order of test execution must also take into account the availability of resources.  For example, the required test tools, test environments or people that may only be available for a specific time window.

##  5.1.6.  Test Pyramid

The [[Test Pyramid | test pyramid]] is a model showing that different tests may have different granularity.  The [[Test Pyramid | test pyramid]] model supports the team in [[Test Automation | test automation]] and in test effort allocation by showing that different [[Test Objective | test objectives]] are supported by different levels of [[Test Automation | test automation]].  The pyramid layers represent groups of tests.  The higher the layer, the lower the test granularity, the lower the test isolation (i.e., the degree of dependency on other elements of the system) and the higher the test execution time.  Tests in the bottom layer are small, isolated, fast, and check a small piece of functionality, so usually a lot of them are needed to achieve a reasonable [[Coverage | coverage]].  The top layer represents complex, high-level, end-to-end tests.  These high-level tests are generally slower than the tests from the lower layers, and they typically check a large piece of functionality, so usually just a few of them are needed to achieve a reasonable level of coverage.  The number and naming of the layers may differ.  For example, the original [[Test Pyramid | test pyramid]] model ([[Succeeding with Agile | Cohn 2009]]) defines three layers: "unit tests", "service tests" and "UI tests".  Another popular model defines [[Component Testing | ]unit (component) tests]], [[Component Integration Testing | integration (component integration) tests]], and end-to-end tests.  Other test levels (see [[2.2.  Test Levels and Test Types#2.2.1. Test Levels | section 2.2.1]]) can also be used.

##  5.1.7.  Testing Quadrants

The [[Testing Quadrants | testing quadrants]], defined by Brian Marick ([[Exploration through Example | Marick 2003]], [[Agile Testing | Crispin 2008]]), group the [[Test Level | test levels]] with the appropriate [[Test Type | test types]], activities, [[Test Technique | test techniques]] and work products in the Agile software development.  The model supports test management in visualizing these to ensure that all appropriate [[Test Type | test types]] and [[Test Level | test levels]] are included in the [[SDLC]] and in understanding that some [[Test Type | test types]] are more relevant to certain [[Test Level | test levels]] than others.  This model also provides a way to differentiate and describe the [[Test Type | test types]] to all stakeholders, including developers, testers, and business representatives.

In this model, tests can be business facing or technology facing.  Tests can also support the team (i.e., guide the development) or critique the product (i.e., measure its behavior against the expectations).  The combination of these two viewpoints determines the four quadrants:

* Quadrant Q1 (technology facing, support the team).  This quadrant contains [[Component Testing | component tests]] and [[Component Integration Testing | component integration tests]].  These tests should be automated and included in the CI process.
* Quadrant Q2 (business facing, support the team).  This quadrant contains [[Functional Testing | functional tests]], examples, user story tests, user experience prototypes, API testing, and simulations.  These tests check the [[Acceptance Criteria | acceptance criteria]] and can be manual or automated.
* Quadrant Q3 (business facing, critique the product).  This quadrant contains [[Exploratory Testing | exploratory testing]], usability testing, user [[Acceptance Testing | acceptance testing]].  These tests are user-oriented and often manual.
* Quadrant Q4 (technology facing, critique the product).  This quadrant contains smoke tests and [[Non-functional Testing | non-functional tests]] (except usability tests).  These tests are often automated.

---
<< [[5.0.  Managing the Test Activities | Previous Section]] | [[5.2.  Risk Management | Next Section]] >>